"""
ä¼˜åŒ–ç‰ˆå‘é‡åŒ–RAGå·¥å…· - å…¨åŠŸèƒ½ç”Ÿäº§çº§ (V2.0)
é›†æˆï¼šCross-Encoder é‡æ’åºã€å¼‚å¸¸å¤„ç†ã€ç¼“å­˜ç®¡ç†ã€é™çº§ç­–ç•¥ã€è¯¦ç»†ç»Ÿè®¡
"""
import json
import numpy as np
from typing import List, Dict, Any, Optional, Union
import logging
from pathlib import Path
import pickle
import hashlib
import time
from functools import lru_cache
import os

# å¯¼å…¥è‡ªå®šä¹‰å¼‚å¸¸ (å‡è®¾å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å®šä¹‰åŸºç¡€å¼‚å¸¸)
try:
    from ..exceptions import (
        KnowledgeBaseNotFoundError, VectorIndexBuildError, 
        SemanticSearchError, RAGException
    )
except ImportError:
    class RAGException(Exception): pass
    class KnowledgeBaseNotFoundError(RAGException): pass
    class VectorIndexBuildError(RAGException): pass
    class SemanticSearchError(RAGException): pass

# ä¾èµ–åº“å¯¼å…¥ä¸ç¯å¢ƒæ£€æŸ¥
try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logging.warning("âš ï¸ sentence-transformersæœªå®‰è£…ï¼Œå‘é‡æ£€ç´¢åŠé‡æ’åºåŠŸèƒ½å—é™ï¼Œå°†ä»…ä½¿ç”¨æ–‡æœ¬åŒ¹é…ã€‚")

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    logging.warning("âš ï¸ faissæœªå®‰è£…ï¼Œå¤§æ•°æ®é‡ä¸‹æ£€ç´¢æ€§èƒ½å¯èƒ½ä¸‹é™ï¼Œå°†ä½¿ç”¨numpyè¿›è¡Œè®¡ç®—ã€‚")


class OptimizedVectorRAGTool:
    """
    å…¨åŠŸèƒ½ç”Ÿäº§çº§ RAG å·¥å…·ç±»
    
    æ ¸å¿ƒèƒ½åŠ›ï¼š
    1. åŒé˜¶æ®µæ£€ç´¢ï¼šBi-Encoder ç²—æ’ + Cross-Encoder ç²¾æ’
    2. æ™ºèƒ½é˜²å¹»è§‰ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å’Œé‡æ’åºåˆ†æ•°çš„åŒé‡é˜ˆå€¼è¿‡æ»¤
    3. å¥å£®æ€§è®¾è®¡ï¼šè‡ªåŠ¨é™çº§ã€å¼‚å¸¸æ•è·ã€ç¼“å­˜é¢„çƒ­ã€è‡ªåŠ¨ç´¢å¼•é‡å»º
    """
    
    def __init__(self, knowledge_base_path: Optional[str] = None, config: Optional[Dict] = None):
        self.config = config or {}
        
        # --- åŸºç¡€è·¯å¾„é…ç½® ---
        self.knowledge_base_path = Path(knowledge_base_path or 
                                       self.config.get('knowledge_base', 'knowledge_base/platform_knowledge.json'))
        self.cache_dir = Path(self.config.get('cache_dir', 'data/vector_cache'))
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            logging.error(f"æ— æ³•åˆ›å»ºç¼“å­˜ç›®å½• {self.cache_dir}: {e}")
        
        # --- æ¨¡å‹é…ç½® (æ ¸å¿ƒå‡çº§ç‚¹) ---
        self.embed_model_name = self.config.get('embed_model', "shibing624/text2vec-base-chinese")
        self.rerank_model_name = self.config.get('rerank_model', "cross-encoder/ms-marco-MiniLM-L-6-v2")
        
        self.embed_model = None
        self.rerank_model = None
        
        # --- æ€§èƒ½ä¸åˆ†å—é…ç½® ---
        self.chunk_size = self.config.get('chunk_size', 300) 
        self.chunk_overlap = self.config.get('chunk_overlap', 50)
        self.retrieve_top_k = self.config.get('retrieve_top_k', 20)  # ç²—æ’å¬å›
        self.final_top_k = self.config.get('top_k', 3)               # ç²¾æ’ç»“æœ
        
        # --- é˜ˆå€¼é…ç½® (é˜²å¹»è§‰å…³é”®) ---
        self.vector_threshold = self.config.get('vector_threshold', 0.35)  
        self.rerank_threshold = self.config.get('rerank_threshold', 0.0) # Sigmoidåé€šå¸¸åœ¨0~1ï¼Œéœ€å¾®è°ƒ
        
        # --- ç¼“å­˜ä¸å·¥ç¨‹é…ç½® ---
        self.lazy_load = self.config.get('lazy_load', True)
        self.cache_ttl = self.config.get('cache_ttl', 3600)
        self.max_cache_size = self.config.get('max_cache_size', 2000)
        
        # --- å†…éƒ¨çŠ¶æ€ ---
        self.knowledge_chunks: List[Dict] = []
        self.embeddings: Optional[np.ndarray] = None
        self.faiss_index = None
        self._initialized = False
        self._initialization_time: Optional[float] = None
        self._query_cache: Dict[str, Dict] = {}
        
        # --- ç»Ÿè®¡ä¿¡æ¯ (è¯¦ç»†ç›‘æ§) ---
        self.stats = {
            'total_searches': 0,
            'cache_hits': 0,
            'vector_searches': 0,
            'fallback_searches': 0,
            'rerank_triggered': 0,
            'avg_search_time': 0.0,
            'initialization_time': 0.0,
            'last_error': None
        }
        
        if not self.lazy_load:
            self._initialize()
    
    def _initialize(self):
        """åˆå§‹åŒ–ç³»ç»Ÿï¼ˆåŒ…å«å¼‚å¸¸å¤„ç†å’Œæ¨¡å‹åŠ è½½ï¼‰"""
        if self._initialized:
            return

        start_time = time.time()
        logging.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿ...")
        
        try:
            # 1. åŠ è½½å‘é‡æ¨¡å‹
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                logging.info(f"æ­£åœ¨åŠ è½½å‘é‡æ¨¡å‹: {self.embed_model_name}")
                try:
                    self.embed_model = SentenceTransformer(self.embed_model_name)
                except Exception as e:
                    logging.error(f"âŒ å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    self.embed_model = None

                # 2. åŠ è½½é‡æ’åºæ¨¡å‹ (æ–°å¢)
                logging.info(f"æ­£åœ¨åŠ è½½é‡æ’åºæ¨¡å‹: {self.rerank_model_name}")
                try:
                    self.rerank_model = CrossEncoder(self.rerank_model_name)
                    logging.info("âœ… é‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logging.warning(f"âš ï¸ é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†è·³è¿‡ç²¾æ’é˜¶æ®µ: {e}")
                    self.rerank_model = None
            else:
                logging.warning("âš ï¸ sentence-transformersä¸å¯ç”¨ï¼Œä»…æ”¯æŒåŸºç¡€æ–‡æœ¬åŒ¹é…")
            
            # 3. åŠ è½½æˆ–æ„å»ºç´¢å¼•
            if self._should_rebuild_index():
                logging.info("æ£€æµ‹åˆ°çŸ¥è¯†åº“æ›´æ–°æˆ–ç¼“å­˜ç¼ºå¤±ï¼Œæ­£åœ¨é‡å»ºç´¢å¼•...")
                self._build_vector_index()
            else:
                logging.info("æ­£åœ¨åŠ è½½ç¼“å­˜ç´¢å¼•...")
                self._load_cached_index()
            
            self._initialized = True
            self._initialization_time = time.time() - start_time
            self.stats['initialization_time'] = self._initialization_time
            logging.info(f"âœ… RAGåˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {self._initialization_time:.2f}ç§’ï¼ŒChunkæ•°: {len(self.knowledge_chunks)}")
            
        except Exception as e:
            # ä¸¥é‡çš„åˆå§‹åŒ–å¤±è´¥éœ€è¦æŠ›å‡ºï¼Œè®©ä¸Šå±‚æ„ŸçŸ¥
            logging.error(f"âŒ RAGåˆå§‹åŒ–ä¸¥é‡å¤±è´¥: {e}", exc_info=True)
            self.stats['last_error'] = str(e)
            if "not exist" in str(e):
                raise KnowledgeBaseNotFoundError(str(self.knowledge_base_path))
            raise RAGException(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    def _ensure_initialized(self):
        """ç¡®ä¿æ‡’åŠ è½½æ¨¡å¼ä¸‹ç³»ç»Ÿå·²åˆå§‹åŒ–"""
        if not self._initialized:
            self._initialize()

    def _should_rebuild_index(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•"""
        cache_file = self.cache_dir / "vector_index.pkl"
        
        # 1. åŸºç¡€æ–‡ä»¶æ£€æŸ¥
        if not cache_file.exists():
            return True
        if not self.knowledge_base_path.exists():
            logging.warning(f"çŸ¥è¯†åº“æ–‡ä»¶ {self.knowledge_base_path} ä¸å­˜åœ¨ï¼Œæ— æ³•å¯¹æ¯”æ—¶é—´æˆ³")
            return False # é¿å…åå¤é‡å»ºç©ºç´¢å¼•
            
        # 2. æ—¶é—´æˆ³å¯¹æ¯”
        try:
            kb_mtime = self.knowledge_base_path.stat().st_mtime
            cache_mtime = cache_file.stat().st_mtime
            return kb_mtime > cache_mtime
        except Exception as e:
            logging.warning(f"æ£€æŸ¥æ–‡ä»¶æ—¶é—´æˆ³å¤±è´¥: {e}ï¼Œé»˜è®¤é‡å»ºç´¢å¼•")
            return True

    def _flatten_json(self, data: Any, meta: Dict = None) -> List[Dict]:
        """é€’å½’æ‰å¹³åŒ–ä»»æ„ JSON ç»“æ„"""
        if meta is None: meta = {}
        documents = []
        
        if isinstance(data, dict):
            for k, v in data.items():
                # è®°å½•è·¯å¾„ä½œä¸ºå…ƒæ•°æ®
                new_meta = meta.copy()
                new_meta['key_path'] = f"{meta.get('key_path', '')}/{k}".strip('/')
                documents.extend(self._flatten_json(v, new_meta))
        elif isinstance(data, list):
            for idx, item in enumerate(data):
                new_meta = meta.copy()
                new_meta['list_index'] = idx
                documents.extend(self._flatten_json(item, new_meta))
        elif isinstance(data, (str, int, float, bool)):
            text = str(data).strip()
            if text:
                documents.append({'content': text, 'metadata': meta})
                
        return documents

    def _chunk_documents(self, documents: List[Dict]) -> List[Dict]:
        """æ–‡æ¡£åˆ†å—å¤„ç† (å¢å¼ºå¥å£®æ€§)"""
        chunks = []
        for doc_idx, doc in enumerate(documents):
            content = doc['content']
            metadata = doc.get('metadata', {})
            
            # è¿‡æ»¤è¿‡çŸ­çš„æ–‡æ¡£
            if len(content) < 5:
                continue
            
            # ç®€å•çš„æ»‘åŠ¨çª—å£åˆ†å—
            if len(content) > self.chunk_size:
                for i in range(0, len(content), self.chunk_size - self.chunk_overlap):
                    chunk_text = content[i:i + self.chunk_size]
                    if len(chunk_text.strip()) < 10: continue
                    
                    chunks.append({
                        'text': chunk_text,
                        'metadata': {
                            **metadata, 
                            'chunk_id': len(chunks),
                            'doc_index': doc_idx,
                            'length': len(chunk_text)
                        },
                        'original_doc': content[:200] + "..." # ç”¨äºæº¯æº
                    })
            else:
                chunks.append({
                    'text': content,
                    'metadata': {**metadata, 'chunk_id': len(chunks), 'doc_index': doc_idx},
                    'original_doc': content
                })
        return chunks

    def _build_vector_index(self):
        """æ„å»ºå‘é‡ç´¢å¼• (å¸¦å®Œæ•´å¼‚å¸¸å¤„ç†)"""
        try:
            logging.info("å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
            
            # åŠ è½½åŸå§‹æ•°æ®
            if not self.knowledge_base_path.exists():
                raise KnowledgeBaseNotFoundError(str(self.knowledge_base_path))
                
            with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
                try:
                    raw_data = json.load(f)
                except json.JSONDecodeError:
                    raise RAGException(f"çŸ¥è¯†åº“æ–‡ä»¶æŸåï¼Œéæœ‰æ•ˆJSON: {self.knowledge_base_path}")
            
            # æ‰å¹³åŒ–æ•°æ®å¤„ç†
            documents = self._flatten_json(raw_data)
            logging.info(f"è§£æå‡º {len(documents)} ä¸ªåŸºç¡€æ–‡æ¡£ç‰‡æ®µ")
            
            # åˆ†å—
            self.knowledge_chunks = self._chunk_documents(documents)
            logging.info(f"åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(self.knowledge_chunks)} ä¸ª chunk")
            
            # å‘é‡åŒ–
            if self.embed_model and self.knowledge_chunks:
                texts = [c['text'] for c in self.knowledge_chunks]
                
                # æ‰¹é‡å¤„ç†ä»¥é˜²å†…å­˜æº¢å‡º
                batch_size = 64
                embeddings_list = []
                total_batches = (len(texts) + batch_size - 1) // batch_size
                
                for i in range(0, len(texts), batch_size):
                    batch = texts[i:i+batch_size]
                    # show_progress_bar=False é¿å…æ—¥å¿—åˆ·å±
                    emb = self.embed_model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
                    embeddings_list.append(emb)
                    if i % (batch_size * 5) == 0:
                        logging.debug(f"å‘é‡åŒ–è¿›åº¦: {i}/{len(texts)}")
                
                if embeddings_list:
                    self.embeddings = np.vstack(embeddings_list)
                    logging.info(f"å‘é‡åŒ–å®Œæˆï¼Œç»´åº¦: {self.embeddings.shape}")
                    
                    # FAISS ç´¢å¼•æ„å»º
                    if FAISS_AVAILABLE:
                        d = self.embeddings.shape[1]
                        # ä½¿ç”¨å†…ç§¯(IP)ç´¢å¼•ï¼Œå‰ææ˜¯å‘é‡å·²å½’ä¸€åŒ–ï¼Œç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦
                        self.faiss_index = faiss.IndexFlatIP(d)
                        faiss.normalize_L2(self.embeddings)
                        self.faiss_index.add(self.embeddings)
                        logging.info("FAISS ç´¢å¼•æ„å»ºæˆåŠŸ")
            else:
                logging.warning("æœªåŠ è½½å‘é‡æ¨¡å‹æˆ–æ— æ–‡æ¡£ï¼Œè·³è¿‡å‘é‡åŒ–æ­¥éª¤")
            
            # ç¼“å­˜
            self._cache_index()
            
        except Exception as e:
            logging.error(f"æ„å»ºç´¢å¼•å¤±è´¥: {e}")
            raise VectorIndexBuildError(self.embed_model_name or "unknown", str(e))

    def _cache_index(self):
        """æŒä¹…åŒ–ç´¢å¼•"""
        try:
            cache_data = {
                'chunks': self.knowledge_chunks, 
                'embeddings': self.embeddings,
                'version': '2.2',
                'timestamp': time.time()
            }
            with open(self.cache_dir / "vector_index.pkl", 'wb') as f:
                pickle.dump(cache_data, f)
            
            if FAISS_AVAILABLE and self.faiss_index:
                faiss.write_index(self.faiss_index, str(self.cache_dir / "faiss_index.bin"))
                
            logging.info(f"ç´¢å¼•å·²ç¼“å­˜è‡³ {self.cache_dir}")
        except Exception as e:
            logging.error(f"ç¼“å­˜ç´¢å¼•å¤±è´¥: {e}")

    def _load_cached_index(self):
        """åŠ è½½ç¼“å­˜ç´¢å¼•"""
        try:
            with open(self.cache_dir / "vector_index.pkl", 'rb') as f:
                data = pickle.load(f)
            
            # ç‰ˆæœ¬æ£€æŸ¥ (å¯é€‰)
            if data.get('version') != '2.2':
                logging.warning("ç¼“å­˜ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œè§¦å‘é‡å»º")
                self._build_vector_index()
                return

            self.knowledge_chunks = data['chunks']
            self.embeddings = data['embeddings']
            
            if FAISS_AVAILABLE:
                idx_path = str(self.cache_dir / "faiss_index.bin")
                if Path(idx_path).exists():
                    self.faiss_index = faiss.read_index(idx_path)
                else:
                    logging.warning("FAISSç´¢å¼•æ–‡ä»¶ç¼ºå¤±ï¼Œå°†é‡å»ºFAISSç´¢å¼•")
                    if self.embeddings is not None:
                         d = self.embeddings.shape[1]
                         self.faiss_index = faiss.IndexFlatIP(d)
                         faiss.normalize_L2(self.embeddings)
                         self.faiss_index.add(self.embeddings)

        except Exception as e:
            logging.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥ï¼Œå°è¯•é‡å»º: {e}")
            self._build_vector_index()

    @lru_cache(maxsize=1000)
    def _get_query_cache_key(self, query: str) -> str:
        """ç”ŸæˆæŸ¥è¯¢æŒ‡çº¹"""
        return hashlib.md5(query.encode('utf-8')).hexdigest()

    def search(self, query: str, top_k: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        æ ¸å¿ƒæœç´¢å…¥å£ï¼šé›†æˆç¼“å­˜ã€å‘é‡æ£€ç´¢ã€Rerank å’Œ é™çº§ç­–ç•¥
        """
        if not query or not isinstance(query, str):
            logging.warning(f"éæ³•æŸ¥è¯¢è¾“å…¥: {query}")
            return []

        start_time = time.time()
        self.stats['total_searches'] += 1
        
        # æ‡’åŠ è½½åˆå§‹åŒ–
        self._ensure_initialized()
        
        target_k = top_k or self.final_top_k
        cache_key = self._get_query_cache_key(query)
        
        # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜ (ä¸€çº§ç¼“å­˜)
        if cache_key in self._query_cache:
            entry = self._query_cache[cache_key]
            if time.time() - entry['time'] < self.cache_ttl:
                self.stats['cache_hits'] += 1
                return entry['results']

        results = []
        try:
            # 2. å‘é‡æ£€ç´¢ (Vector Search - ç²—æ’)
            if self.embed_model and self.knowledge_chunks:
                # å¬å› retrieve_top_k (æ¯”å¦‚20ä¸ª) ç»™ Reranker
                candidates = self._vector_search(query, self.retrieve_top_k)
                self.stats['vector_searches'] += 1
                
                # 3. é‡æ’åº (Rerank - ç²¾æ’)
                if candidates and self.rerank_model:
                    self.stats['rerank_triggered'] += 1
                    results = self._rerank_search(query, candidates, target_k)
                else:
                    # å¦‚æœæ²¡æœ‰ Rerankerï¼Œç›´æ¥æˆªå–
                    results = candidates[:target_k]
                    if not self.rerank_model and candidates:
                        logging.debug("æœªå¯ç”¨Rerankï¼Œç›´æ¥è¿”å›å‘é‡æ£€ç´¢ç»“æœ")
            else:
                # 4. é™çº§æœç´¢ (Fallback - å…³é”®è¯åŒ¹é…)
                logging.info("å‘é‡æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨æ–‡æœ¬åŒ¹é…é™çº§æœç´¢")
                results = self._fallback_search(query)
                self.stats['fallback_searches'] += 1
            
            # 5. æ›´æ–°ç¼“å­˜
            if results:
                self._query_cache[cache_key] = {
                    'results': results,
                    'time': time.time()
                }
                self._cleanup_cache()
            
            # æ›´æ–°ç»Ÿè®¡è€—æ—¶
            elapsed = time.time() - start_time
            self._update_avg_time(elapsed)
            
            return results
            
        except Exception as e:
            logging.error(f"æœç´¢è¿‡ç¨‹å¼‚å¸¸: {e}", exc_info=True)
            self.stats['last_error'] = str(e)
            # æœ€åçš„ä¿åº•ï¼šå°è¯•æ–‡æœ¬åŒ¹é…
            return self._fallback_search(query)

    def _vector_search(self, query: str, k: int) -> List[Dict]:
        """æ‰§è¡Œå‘é‡æ£€ç´¢"""
        try:
            query_vec = self.embed_model.encode([query])
            
            if FAISS_AVAILABLE and self.faiss_index:
                faiss.normalize_L2(query_vec)
                D, I = self.faiss_index.search(query_vec, k)
                candidates = []
                for score, idx in zip(D[0], I[0]):
                    if idx != -1 and score > self.vector_threshold:
                        candidates.append({
                            'chunk': self.knowledge_chunks[idx],
                            'score': float(score),
                            'source': 'vector_faiss'
                        })
                return candidates
            elif self.embeddings is not None:
                # Numpy å®ç°
                scores = np.dot(self.embeddings, query_vec.T).flatten()
                top_idxs = np.argsort(scores)[::-1][:k]
                return [
                    {
                        'chunk': self.knowledge_chunks[i], 
                        'score': float(scores[i]),
                        'source': 'vector_numpy'
                    }
                    for i in top_idxs if scores[i] > self.vector_threshold
                ]
            else:
                return []
        except Exception as e:
            logging.error(f"å‘é‡æ£€ç´¢è®¡ç®—å¤±è´¥: {e}")
            return []

    def _rerank_search(self, query: str, candidates: List[Dict], top_k: int) -> List[Dict]:
        """æ‰§è¡Œé‡æ’åº"""
        if not candidates: return []
        
        try:
            # æ„é€  (Query, Doc) å¯¹
            pairs = [[query, c['chunk']['text']] for c in candidates]
            
            # é¢„æµ‹åˆ†æ•°
            scores = self.rerank_model.predict(pairs)
            
            final_results = []
            for i, score in enumerate(scores):
                # å¤„ç†åˆ†æ•°ï¼šå…¼å®¹ Logits å’Œ Sigmoid è¾“å‡º
                # å¤§å¤šæ•° Reranker è¾“å‡ºæœªå½’ä¸€åŒ–çš„ logitsï¼Œè¿™é‡Œç®€å•è½¬æ¢
                # æˆ–è€…ç›´æ¥ç”¨ raw score æ’åºå³å¯ï¼Œé˜ˆå€¼éœ€è¦å¯¹åº”è°ƒæ•´
                normalized_score = float(score) 
                
                # Rerank é˜ˆå€¼è¿‡æ»¤ (æ ¸å¿ƒé˜²å¹»è§‰ç‚¹ï¼šæ— å…³çš„ç›´æ¥ä¸¢å¼ƒ)
                if normalized_score > self.rerank_threshold:
                    cand = candidates[i]
                    final_results.append({
                        'text': cand['chunk']['text'],
                        'metadata': cand['chunk']['metadata'],
                        'similarity': cand['score'], # ä¿ç•™åŸå§‹å‘é‡åˆ†
                        'rerank_score': normalized_score,
                        'source': 'reranked',
                        'id': cand['chunk']['metadata'].get('chunk_id')
                    })
            
            # æŒ‰ Rerank åˆ†æ•°å€’åº
            final_results.sort(key=lambda x: x['rerank_score'], reverse=True)
            return final_results[:top_k]
            
        except Exception as e:
            logging.error(f"é‡æ’åºè®¡ç®—å¤±è´¥: {e}")
            # é™çº§ï¼šå¦‚æœ Rerank å¤±è´¥ï¼Œè¿”å›åŸå§‹å‘é‡ç»“æœ
            return [
                {**c['chunk'], 'similarity': c['score'], 'rank_score': 0, 'source': 'vector_fallback'} 
                for c in candidates[:top_k]
            ]

    def _fallback_search(self, query: str) -> List[Dict]:
        """é™çº§ï¼šç®€å•çš„æ–‡æœ¬åŒ…å«åŒ¹é…"""
        results = []
        q_lower = query.lower()
        for chunk in self.knowledge_chunks:
            # ç®€å•çš„å…³é”®è¯å‘½ä¸­è®¡åˆ†
            if q_lower in chunk['text'].lower():
                results.append({
                    'text': chunk['text'],
                    'metadata': chunk['metadata'],
                    'similarity': 1.0,
                    'source': 'text_match_fallback'
                })
        return results[:self.final_top_k]

    def _cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        if len(self._query_cache) > self.max_cache_size:
            now = time.time()
            # ä¼˜å…ˆåˆ è¿‡æœŸçš„
            keys_to_del = [k for k, v in self._query_cache.items() if now - v['time'] > self.cache_ttl]
            
            if not keys_to_del:
                # å¦‚æœæ²¡è¿‡æœŸçš„ï¼Œåˆ æœ€æ—§çš„ (FIFO)
                sorted_keys = sorted(self._query_cache.keys(), key=lambda k: self._query_cache[k]['time'])
                keys_to_del = sorted_keys[:int(self.max_cache_size * 0.2)] # åˆ æ‰ 20%
                
            for k in keys_to_del: 
                del self._query_cache[k]

    def _update_avg_time(self, new_time):
        n = self.stats['total_searches']
        self.stats['avg_search_time'] = (self.stats['avg_search_time'] * (n-1) + new_time) / n

    def get_stats(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†è¿è¡Œç»Ÿè®¡"""
        return {
            **self.stats,
            'index_size': len(self.knowledge_chunks),
            'has_embedding_model': self.embed_model is not None,
            'has_reranker_model': self.rerank_model is not None,
            'faiss_enabled': FAISS_AVAILABLE and self.faiss_index is not None,
            'cache_size': len(self._query_cache),
            'config': {
                'chunk_size': self.chunk_size,
                'top_k': self.final_top_k,
                'rerank_threshold': self.rerank_threshold
            }
        }

    def warmup_cache(self, queries: List[str]):
        """ç¼“å­˜é¢„çƒ­"""
        logging.info(f"å¼€å§‹é¢„çƒ­ {len(queries)} ä¸ªæŸ¥è¯¢...")
        start = time.time()
        for i, q in enumerate(queries):
            try:
                self.search(q)
                if i % 10 == 0: logging.debug(f"é¢„çƒ­è¿›åº¦: {i}/{len(queries)}")
            except Exception as e:
                logging.warning(f"é¢„çƒ­æŸ¥è¯¢å¤±è´¥ '{q}': {e}")
        logging.info(f"é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {time.time() - start:.2f}s")

    def clear_cache(self):
        self._query_cache.clear()
        logging.info("ç¼“å­˜å·²æ¸…ç©º")

# --- æœ¬åœ°æµ‹è¯•ä»£ç  (ä¿ç•™ï¼Œæ–¹ä¾¿è°ƒè¯•) ---
if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # æ¨¡æ‹Ÿé…ç½®
    test_config = {
        'chunk_size': 100,
        'top_k': 3,
        'lazy_load': False
    }
    
    # åˆ›å»ºå·¥å…·å®ä¾‹
    tool = OptimizedVectorRAGTool(config=test_config)
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = ["APIè°ƒç”¨é™åˆ¶", "ç³»ç»Ÿç¨³å®šæ€§", "å¦‚ä½•é‡ç½®å¯†ç "]
    
    print("\n" + "="*50)
    print("ğŸ” RAG å·¥å…·æµ‹è¯•å¼€å§‹")
    print("="*50)
    
    for q in queries:
        print(f"\nâ“ æŸ¥è¯¢: {q}")
        results = tool.search(q)
        for i, res in enumerate(results):
            score_key = 'rerank_score' if 'rerank_score' in res else 'similarity'
            print(f"  [{i+1}] Score: {res.get(score_key, 0):.4f} | Source: {res.get('source')} | Text: {res['text'][:50]}...")
            
    # æ‰“å°ç»Ÿè®¡
    print("\n" + "="*50)
    print("ğŸ“Š è¿è¡Œç»Ÿè®¡:")
    print(json.dumps(tool.get_stats(), indent=2, ensure_ascii=False))
