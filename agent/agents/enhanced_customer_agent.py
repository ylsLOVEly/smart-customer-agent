"""
å¢å¼ºç‰ˆæ™ºèƒ½å®¢æœAgent - é›†æˆæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½
åŒ…å«å‘é‡åŒ–RAGã€Prometheusç›‘æ§ã€é«˜çº§ç¼“å­˜ç­‰å®Œæ•´åŠŸèƒ½
"""
import asyncio
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
import time

# å¯¼å…¥åŸºç¡€å·¥å…·
from agent.tools.feishu_tool import FeishuTool
from agent.tools.apifox_tool import ApifoxTool
from agent.tools.monitor_tool import MonitorTool

# å¯¼å…¥å¢å¼ºå·¥å…·
from agent.tools.vector_rag_tool import VectorRAGTool
from agent.tools.metrics_tool import MetricsTool, record_request, record_cache_hit, record_error
from agent.tools.advanced_cache_tool import AdvancedCacheManager, cache

# å¯¼å…¥æ¨¡å‹å®¢æˆ·ç«¯
from agent.models.deepseek_client import DeepSeekClient

class EnhancedCustomerServiceAgent:
    """å¢å¼ºç‰ˆæ™ºèƒ½å®¢æœç›‘æ§Agent"""
    
    def __init__(self, config: Dict = None):
        self.config = config or {}
        
        # åˆå§‹åŒ–æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = DeepSeekClient()
        
        # åˆå§‹åŒ–å¢å¼ºå·¥å…·
        self._init_enhanced_tools()
        
        # åˆå§‹åŒ–åŸºç¡€å·¥å…·
        self.feishu_tool = FeishuTool()
        self.apifox_tool = ApifoxTool()
        self.monitor_tool = MonitorTool()
        
        self.logger.info("å¢å¼ºç‰ˆæ™ºèƒ½å®¢æœAgentåˆå§‹åŒ–å®Œæˆ")
    
    def _init_enhanced_tools(self):
        """åˆå§‹åŒ–å¢å¼ºå·¥å…·"""
        # ç¼“å­˜é…ç½®
        cache_config = {
            'memory_max_size': 50 * 1024 * 1024,  # 50MB
            'disk_max_size': 500 * 1024 * 1024,   # 500MB
            'default_ttl': 1800,  # 30åˆ†é’Ÿ
            'cleanup_interval': 300,  # 5åˆ†é’Ÿæ¸…ç†é—´éš”
            'cache_dir': 'data/agent_cache',
            'redis': {
                'enabled': False,  # é»˜è®¤ç¦ç”¨Redisï¼Œå¯åœ¨é…ç½®ä¸­å¯ç”¨
                'host': 'localhost',
                'port': 6379,
                'db': 1
            }
        }
        cache_config.update(self.config.get('cache', {}))
        self.cache_manager = AdvancedCacheManager(cache_config)
        
        # å‘é‡åŒ–RAGå·¥å…·
        knowledge_path = self.config.get('knowledge_base', 'knowledge_base/platform_knowledge.json')
        self.vector_rag = VectorRAGTool(knowledge_path)
        
        # ç›‘æ§å·¥å…·
        self.metrics = MetricsTool()
        
        self.logger.info("å¢å¼ºå·¥å…·åˆå§‹åŒ–å®Œæˆ")
    
    @cache(ttl=900, priority='high')  # ç¼“å­˜15åˆ†é’Ÿ
    def _enhanced_knowledge_search(self, query: str) -> List[Dict[str, Any]]:
        """å¢å¼ºçš„çŸ¥è¯†æ£€ç´¢ - ä½¿ç”¨å‘é‡åŒ–RAG"""
        start_time = time.time()
        
        try:
            # ä½¿ç”¨å‘é‡åŒ–æœç´¢
            results = self.vector_rag.search(query, top_k=3)
            
            # è®°å½•ç›‘æ§æŒ‡æ ‡
            search_time = time.time() - start_time
            record_request('knowledge_search', 'success', search_time, 'vector_rag')
            
            if results:
                record_cache_hit('knowledge_search')
                self.logger.info(f"å‘é‡åŒ–æ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ°{len(results)}ä¸ªç›¸å…³ç»“æœ")
            else:
                self.logger.warning(f"å‘é‡åŒ–æ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³ç»“æœ: {query}")
            
            return results
            
        except Exception as e:
            record_error('knowledge_search_error', details=str(e))
            self.logger.error(f"å‘é‡åŒ–æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def _generate_reply(self, user_query: str, context: Dict) -> str:
        """ç”Ÿæˆæ™ºèƒ½å›å¤ - é›†æˆç¼“å­˜å’Œç›‘æ§"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"reply:{hash(user_query + str(context))}"
            cached_reply = self.cache_manager.get(cache_key)
            
            if cached_reply:
                record_cache_hit('reply_generation')
                self.logger.info("ä½¿ç”¨ç¼“å­˜å›å¤")
                return cached_reply
            
            # å¢å¼ºçš„çŸ¥è¯†æ£€ç´¢
            knowledge_results = self._enhanced_knowledge_search(user_query)
            
            # æ„å»ºå¢å¼ºçš„Prompt
            knowledge_context = ""
            if knowledge_results:
                knowledge_context = "\n".join([
                    f"ç›¸å…³ä¿¡æ¯{i+1}ï¼ˆç›¸ä¼¼åº¦: {r['similarity']:.2f}ï¼‰: {r['text']}"
                    for i, r in enumerate(knowledge_results)
                ])
            
            enhanced_prompt = f"""
ä½œä¸ºæ™ºèƒ½å®¢æœï¼ŒåŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

ç›¸å…³çŸ¥è¯†ï¼š
{knowledge_context}

ç³»ç»ŸçŠ¶æ€ï¼š
- APIçŠ¶æ€: {context.get('api_status', 'æœªçŸ¥')}
- å“åº”æ—¶é—´: {context.get('api_response_time', 'æœªçŸ¥')}
- ç›‘æ§æ—¥å¿—: {context.get('monitor_log', [])}

ğŸš« ä¸¥æ ¼çº¦æŸï¼š
1. å¦‚æœAPIçŠ¶æ€åŒ…å«500/503/errorï¼Œç»ä¸èƒ½è¯´"ç³»ç»Ÿæ­£å¸¸"
2. å¦‚æœç›‘æ§æ—¥å¿—æœ‰Error/Criticalï¼Œå¿…é¡»å¦‚å®å‘ŠçŸ¥
3. åŸºäºçœŸå®ç›‘æ§æ•°æ®ï¼Œä¸å¾—ç¼–é€ ç³»ç»ŸçŠ¶æ€
4. ã€é‡è¦ã€‘æ£€æµ‹åˆ° monitor_log ä¸­å­˜åœ¨ Error æˆ– API çŠ¶æ€é 200 æ—¶ï¼Œå¿…é¡»åœ¨å›å¤å¼€å¤´æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ç³»ç»Ÿå‡ºç°å¼‚å¸¸ï¼Œç¦æ­¢æ©ç›–æ•…éšœ

âœ… å›å¤è¦æ±‚ï¼š
1. å¦‚æœçŸ¥è¯†åº“æœ‰ç›¸å…³ä¿¡æ¯ï¼ŒåŸºäºçŸ¥è¯†åº“å›ç­”
2. å¦‚æœæ¶‰åŠç³»ç»ŸçŠ¶æ€é—®é¢˜ï¼Œè¯šå®å‘ŠçŸ¥çœŸå®æƒ…å†µ
3. å¦‚æœçŸ¥è¯†åº“æ— ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜å¹¶å»ºè®®è”ç³»å®¢æœ
4. ä¿æŒä¸“ä¸šã€å‹å–„çš„è¯­è°ƒ

è¯·æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›å¤ï¼š
"""
            
            # è°ƒç”¨LLMç”Ÿæˆå›å¤
            reply = await self.llm_client.call_model(
                model="deepseek/deepseek-v3.2-think",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šæ™ºèƒ½å®¢æœï¼Œæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"},
                    {"role": "user", "content": enhanced_prompt}
                ],
                temperature=0.7,
                expected_format='text'
            )
            
            # ç¼“å­˜å›å¤
            self.cache_manager.set(
                cache_key, reply, ttl=1800, priority='normal',
                metadata={
                    'user_query': user_query,
                    'knowledge_results_count': len(knowledge_results),
                    'generated_at': datetime.now().isoformat()
                }
            )
            
            # è®°å½•ç›‘æ§æŒ‡æ ‡
            generation_time = time.time() - start_time
            record_request('reply_generation', 'success', generation_time, 'enhanced_agent')
            
            return reply
            
        except Exception as e:
            generation_time = time.time() - start_time
            record_request('reply_generation', 'error', generation_time, 'enhanced_agent')
            record_error('reply_generation_error', details=str(e))
            
            self.logger.error(f"å›å¤ç”Ÿæˆå¤±è´¥: {e}")
            return "ç³»ç»Ÿé‡åˆ°ä¸´æ—¶é—®é¢˜ï¼Œæ­£åœ¨è‡ªåŠ¨ä¿®å¤ä¸­ï¼Œè¯·ç¨åé‡è¯•ã€‚å¦‚éœ€ç´§æ€¥å¸®åŠ©ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
    
    def _should_trigger_alert(self, case_data: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘å‘Šè­¦ - å¢å¼ºåˆ¤æ–­é€»è¾‘"""
        api_status = case_data.get('api_status', '')
        monitor_log = case_data.get('monitor_log', [])
        
        # APIçŠ¶æ€å¼‚å¸¸
        if 'error' in api_status.lower() or '500' in api_status or '503' in api_status:
            return True
        
        # ç›‘æ§æ—¥å¿—æœ‰é”™è¯¯
        if monitor_log:
            for log_entry in monitor_log:
                if log_entry.get('status') in ['Error', 'Critical']:
                    return True
        
        return False
    
    async def _trigger_alerts(self, case_id: str, case_data: Dict) -> List[Dict]:
        """è§¦å‘å‘Šè­¦ - é›†æˆç›‘æ§"""
        actions = []
        start_time = time.time()
        
        try:
            # å‘é€é£ä¹¦å‘Šè­¦
            feishu_result = await self.feishu_tool.send_alert(case_data)
            if feishu_result:
                actions.append({"feishu_webhook": "Sent success (Enhanced)"})
            
            # åˆ›å»ºApifoxæ–‡æ¡£ï¼ˆå¼‚æ­¥è°ƒç”¨ï¼‰
            apifox_result = await self.apifox_tool.create_error_doc(case_id, case_data)
            if apifox_result:
                actions.append({"apifox_doc_id": f"DOC_{datetime.now().strftime('%Y%m%d')}_{case_id}"})
            
            # è®°å½•ç›‘æ§æŒ‡æ ‡
            alert_time = time.time() - start_time
            record_request('alert_processing', 'success', alert_time, 'enhanced_agent')
            
            # æ›´æ–°ç³»ç»ŸçŠ¶æ€
            self.metrics.update_system_status('alert_system', True)
            
        except Exception as e:
            alert_time = time.time() - start_time
            record_request('alert_processing', 'error', alert_time, 'enhanced_agent')
            record_error('alert_error', details=str(e))
            
            self.logger.error(f"å‘Šè­¦å¤„ç†å¤±è´¥: {e}")
        
        return actions
    
    async def process_case(self, case_data: Dict) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ¡ˆä¾‹ - å®Œæ•´å¢å¼ºæµç¨‹"""
        case_id = case_data.get('case_id', 'UNKNOWN')
        user_query = case_data.get('user_query', '')
        
        self.logger.info(f"å¤„ç†æ¡ˆä¾‹ {case_id}: {user_query[:50]}...")
        
        start_time = time.time()
        result = {
            'case_id': case_id,
            'reply': '',
            'action_triggered': None
        }
        
        try:
            # ç”Ÿæˆæ™ºèƒ½å›å¤
            reply = await self._generate_reply(user_query, case_data)
            result['reply'] = reply
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘å‘Šè­¦
            if self._should_trigger_alert(case_data):
                actions = await self._trigger_alerts(case_id, case_data)
                if actions:
                    result['action_triggered'] = actions
                
                self.logger.warning(f"æ¡ˆä¾‹ {case_id} è§¦å‘å‘Šè­¦ï¼Œæ‰§è¡Œäº† {len(actions)} ä¸ªåŠ¨ä½œ")
            
            # è®°å½•æˆåŠŸæŒ‡æ ‡
            process_time = time.time() - start_time
            record_request('case_processing', 'success', process_time, 'enhanced_agent')
            
            # æ›´æ–°æ€§èƒ½è¯„åˆ†
            self.metrics.update_performance_score('case_processing', min(1.0, 3.0/process_time))
            
        except Exception as e:
            process_time = time.time() - start_time
            record_request('case_processing', 'error', process_time, 'enhanced_agent')
            record_error('case_processing_error', model='enhanced_agent', details=str(e))
            
            self.logger.error(f"æ¡ˆä¾‹ {case_id} å¤„ç†å¤±è´¥: {e}")
            result['reply'] = "ç³»ç»Ÿé‡åˆ°ä¸´æ—¶é—®é¢˜ï¼Œæ­£åœ¨è‡ªåŠ¨ä¿®å¤ä¸­ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        
        return result
    
    async def process_batch(self, input_file: str, output_file: str) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ¡ˆä¾‹ - å®Œæ•´å·¥ä½œæµç¨‹"""
        self.logger.info("å¼€å§‹æ‰¹é‡å¤„ç†æ¡ˆä¾‹")
        
        try:
            # è¯»å–è¾“å…¥æ•°æ®
            with open(input_file, 'r', encoding='utf-8') as f:
                cases = json.load(f)
            
            self.logger.info(f"è¯»å–åˆ° {len(cases)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
            
            # æ‰¹é‡å¤„ç†
            results = []
            alert_count = 0
            
            for i, case_data in enumerate(cases, 1):
                self.logger.info(f"å¤„ç†æ¡ˆä¾‹ {i}/{len(cases)}: {case_data.get('case_id')}")
                
                result = await self.process_case(case_data)
                results.append(result)
                
                if result.get('action_triggered'):
                    alert_count += 1
            
            # ä¿å­˜ç»“æœ
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
            report = {
                'summary': {
                    'total_cases': len(cases),
                    'successful_cases': len(results),
                    'alerts_triggered': alert_count,
                    'output_file': str(output_path.absolute())
                },
                'metrics': self.metrics.get_metrics_summary(),
                'cache_stats': self.cache_manager.get_stats(),
                'rag_stats': self.vector_rag.get_stats() if hasattr(self.vector_rag, 'get_stats') else {},
                'processing_completed_at': datetime.now().isoformat()
            }
            
            self.logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼")
            self.logger.info(f"å¤„ç†æ¡ˆä¾‹: {len(results)}/{len(cases)}")
            self.logger.info(f"è§¦å‘å‘Šè­¦: {alert_count}")
            self.logger.info(f"ç»“æœä¿å­˜: {output_path.absolute()}")
            
            return report
            
        except Exception as e:
            record_error('batch_processing_error', details=str(e))
            self.logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            raise
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå®Œæ•´çŠ¶æ€"""
        return {
            'agent_info': {
                'type': 'EnhancedCustomerServiceAgent',
                'version': '2.0.0',
                'capabilities': [
                    'vector_rag_search',
                    'advanced_caching',
                    'prometheus_metrics',
                    'intelligent_alerting',
                    'multi_model_backup'
                ]
            },
            'llm_status': {
                'available_models': [
                    "deepseek/deepseek-v3.2",
                    "deepseek/deepseek-v3.2-think", 
                    "deepseek/deepseek-v3.1"
                ],
                'current_model': 'deepseek/deepseek-v3.2-think',
                'network_status': self.llm_client.get_network_status()
            },
            'metrics': self.metrics.get_metrics_summary(),
            'cache_stats': self.cache_manager.get_stats(),
            'rag_stats': self.vector_rag.get_stats() if hasattr(self.vector_rag, 'get_stats') else {},
            'system_time': datetime.now().isoformat()
        }
    
    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        status = self.get_system_status()
        
        report = f"""
# å¢å¼ºç‰ˆæ™ºèƒ½å®¢æœAgentæ€§èƒ½æŠ¥å‘Š

## ç³»ç»Ÿä¿¡æ¯
- Agentç‰ˆæœ¬: {status['agent_info']['version']}
- ç”Ÿæˆæ—¶é—´: {status['system_time']}
- æ ¸å¿ƒèƒ½åŠ›: {', '.join(status['agent_info']['capabilities'])}

## æ¨¡å‹çŠ¶æ€
- å½“å‰æ¨¡å‹: {status['llm_status']['current_model']}
- å¤‡ç”¨æ¨¡å‹: {len(status['llm_status']['available_models'])}ä¸ª
- ç½‘ç»œçŠ¶æ€: {status['llm_status']['network_status']['status']}

## æ€§èƒ½æŒ‡æ ‡
{self.metrics.generate_report()}

## ç¼“å­˜ç»Ÿè®¡
- å‘½ä¸­ç‡: {status['cache_stats']['hit_rate']}%
- å†…å­˜ä½¿ç”¨: {status['cache_stats']['size_info']['memory_usage']} / {status['cache_stats']['size_info']['memory_limit']}
- Rediså¯ç”¨: {status['cache_stats']['config']['redis_available']}

## RAGç»Ÿè®¡
- æ¨¡å‹å¯ç”¨: {status['rag_stats'].get('model_available', False)}
- çŸ¥è¯†å—æ•°é‡: {status['rag_stats'].get('chunks_count', 0)}
- FAISSç´¢å¼•: {status['rag_stats'].get('faiss_available', False)}

---
æŠ¥å‘Šç”Ÿæˆå®Œæ¯• ğŸš€
        """
        
        return report.strip()

# æµ‹è¯•å’Œæ¼”ç¤ºå‡½æ•°
async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå¢å¼ºAgentåŠŸèƒ½"""
    print("=" * 60)
    print("å¢å¼ºç‰ˆæ™ºèƒ½å®¢æœAgent - å®Œæ•´åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºAgentå®ä¾‹
    config = {
        'knowledge_base': '../data/inputs.json',  # ä½¿ç”¨æµ‹è¯•æ•°æ®ä½œä¸ºçŸ¥è¯†åº“
        'cache': {
            'memory_max_size': 20 * 1024 * 1024,  # 20MB
            'default_ttl': 600  # 10åˆ†é’Ÿ
        }
    }
    
    agent = EnhancedCustomerServiceAgent(config)
    
    # å¤„ç†æµ‹è¯•æ¡ˆä¾‹
    input_file = '../data/inputs.json'
    output_file = '../data/outputs/enhanced_results.json'
    
    try:
        report = await agent.process_batch(input_file, output_file)
        
        print("\n" + "=" * 60)
        print("å¤„ç†å®Œæˆï¼ä»¥ä¸‹æ˜¯è¯¦ç»†æŠ¥å‘Šï¼š")
        print("=" * 60)
        
        # æ˜¾ç¤ºå¤„ç†æ‘˜è¦
        summary = report['summary']
        print(f"ğŸ“Š æ€»è®¡å¤„ç†: {summary['total_cases']}ä¸ªæ¡ˆä¾‹")
        print(f"âœ… æˆåŠŸå¤„ç†: {summary['successful_cases']}ä¸ªæ¡ˆä¾‹")
        print(f"ğŸš¨ è§¦å‘å‘Šè­¦: {summary['alerts_triggered']}ä¸ªæ¡ˆä¾‹")
        print(f"ğŸ’¾ ç»“æœæ–‡ä»¶: {summary['output_file']}")
        
        # æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
        print("\n" + agent.generate_performance_report())
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        logging.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())
