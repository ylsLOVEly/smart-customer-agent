"""
å¢å¼ºç‰ˆæ™ºèƒ½å®¢æœAgent - ç”Ÿäº§æ²»ç†æ——èˆ°ç‰ˆ (V5.4)
é›†æˆï¼š
1. æ™ºèƒ½åˆ†çº§è·¯ç”± (EnhancedRouter): è§„åˆ™/æ¨¡å‹/ç½®ä¿¡åº¦ä¸‰çº§åˆ¤æ–­
2. å·®åˆ†å¹¶å‘æ§åˆ¶ (SmartConcurrency): ç®€å•/å¤æ‚è¯·æ±‚éš”ç¦»å¹¶å‘æ± 
3. é«˜æ€§èƒ½å¼‚æ­¥ RAG: å¼‚æ­¥ç¼“å­˜å±‚ + çº¿ç¨‹æ± å‘é‡è®¡ç®— (Cache-First Strategy)
4. å…¨é“¾è·¯é˜²å¹»è§‰: Vector RAG -> Rerank -> LLM -> Judge -> Fallback
5. ä¼ä¸šçº§æ²»ç†: ä¸¥æ ¼é…ç½®æ ¡éªŒã€æ‡’åŠ è½½æ„ŸçŸ¥çš„å¥åº·æ£€æŸ¥ã€ä¾èµ–æ³¨å…¥æµ‹è¯•
"""
import asyncio
import json
import logging
import time
import re
import traceback
from pathlib import Path
from typing import Dict, Any, List, Optional, Literal, Union
from datetime import datetime
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor

# --- åŸºç¡€å·¥å…·å¯¼å…¥ ---
from agent.tools.feishu_tool import FeishuTool
from agent.tools.apifox_tool import ApifoxTool
from agent.tools.monitor_tool import MonitorTool

# --- å¢å¼ºå·¥å…·å¯¼å…¥ ---
from agent.tools.optimized_vector_rag_tool import OptimizedVectorRAGTool
from agent.tools.metrics_tool import MetricsTool, record_request, record_error
from agent.tools.advanced_cache_tool import AdvancedCacheManager
from agent.models.deepseek_client import DeepSeekClient

# --- é…ç½®å¯¼å…¥ (å¸¦å®¹é”™) ---
try:
    from config.prompts import (
        SYSTEM_PROMPT, ENHANCED_RAG_PROMPT, VERIFY_PROMPT, INTENT_ROUTER_PROMPT
    )
except ImportError:
    # ç¾éš¾æ¢å¤é…ç½®
    SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å®¢æœ..."
    ENHANCED_RAG_PROMPT = "{context_str}\n{query}"
    VERIFY_PROMPT = "éªŒè¯: {response} æ˜¯å¦åŸºäº {context}"
    INTENT_ROUTER_PROMPT = "åˆ¤æ–­æ„å›¾: {query}"

# ==========================================
# æ²»ç†ç»„ä»¶ (Validation & Governance)
# ==========================================

class ConfigValidator:
    """é…ç½®æ ¡éªŒå™¨ - ç¡®ä¿æœåŠ¡å¯åŠ¨æ—¶çš„é…ç½®åˆæ³•æ€§"""
    
    # å…è®¸çš„æ¨¡å‹åˆ—è¡¨ (ç™½åå•)
    ALLOWED_MODELS = {
        'deepseek/deepseek-v3.2',
        'deepseek/deepseek-v3.2-think',
        'deepseek/deepseek-v3.1',
        # æµ‹è¯•ç”¨ Mock æ¨¡å‹åç§°
        'mock_router', 'mock_simple', 'mock_complex', 'mock_verifier'
    }

    @staticmethod
    def validate(config: Dict) -> None:
        errors = []
        
        # 1. å¹¶å‘é™åˆ¶éªŒè¯
        simple_limit = config.get('concurrency_simple', 20)
        complex_limit = config.get('concurrency_complex', 5)
        
        if not isinstance(simple_limit, int) or not (0 < simple_limit <= 1000):
            errors.append(f"simpleå¹¶å‘é™åˆ¶å¿…é¡»åœ¨1-1000ä¹‹é—´ï¼Œå½“å‰å€¼: {simple_limit}")
        if not isinstance(complex_limit, int) or not (0 < complex_limit <= 100):
            errors.append(f"complexå¹¶å‘é™åˆ¶å¿…é¡»åœ¨1-100ä¹‹é—´ï¼Œå½“å‰å€¼: {complex_limit}")
        
        # 2. æ¨¡å‹é…ç½®éªŒè¯
        models = config.get('models', {})
        for role, model_name in models.items():
            if not model_name:
                errors.append(f"æ¨¡å‹é…ç½® '{role}' ä¸èƒ½ä¸ºç©º")
            elif model_name not in ConfigValidator.ALLOWED_MODELS:
                # ç”Ÿäº§ç¯å¢ƒå»ºè®®å¼€å¯ä¸¥æ ¼æ£€æŸ¥ï¼Œæˆ–è€…æ”¹ä¸º warning
                logging.warning(f"âš ï¸ è­¦å‘Š: æ¨¡å‹ '{model_name}' (ç”¨äº {role}) ä¸åœ¨æ¨èç™½åå•ä¸­")
        
        # 3. RAGé…ç½®éªŒè¯
        rag_conf = config.get('rag_config', {})
        top_k = rag_conf.get('top_k', 3)
        if not isinstance(top_k, int) or not (0 < top_k <= 100):
            errors.append(f"RAG top_k å¿…é¡»åœ¨1-100ä¹‹é—´ï¼Œå½“å‰å€¼: {top_k}")
        
        if 'rerank_threshold' in rag_conf:
            rt = rag_conf['rerank_threshold']
            if not (0 <= rt <= 1):
                errors.append(f"rerank_threshold å¿…é¡»åœ¨ 0-1 ä¹‹é—´ï¼Œå½“å‰å€¼: {rt}")
        
        # 4. ç¼“å­˜é…ç½®éªŒè¯
        cache_conf = config.get('cache', {})
        if 'default_ttl' in cache_conf and cache_conf['default_ttl'] < 0:
            errors.append("ç¼“å­˜ TTL ä¸èƒ½ä¸ºè´Ÿæ•°")

        if errors:
            raise ValueError(f"âŒ é…ç½®æ ¡éªŒå¤±è´¥: {'; '.join(errors)}")

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·"""
    @staticmethod
    async def run_benchmark(agent_instance, test_cases: List[Dict], concurrency_limit: int = 10) -> Dict:
        """æ‰§è¡ŒåŸºå‡†æµ‹è¯•å¹¶è¿”å›è¯¦ç»†æŒ‡æ ‡"""
        if not test_cases:
            return {"error": "No test cases provided"}

        results = {
            'latency': {'min': 0.0, 'max': 0.0, 'avg': 0.0, 'p95': 0.0},
            'throughput': 0.0,
            'success_rate': 0.0,
            'samples': len(test_cases),
            'concurrency': concurrency_limit
        }
        
        start_time = time.time()
        
        # ä½¿ç”¨ç‹¬ç«‹çš„ Semaphore æ§åˆ¶å‹æµ‹å¹¶å‘åº¦
        semaphore = asyncio.Semaphore(concurrency_limit)
        
        async def bounded_process(case):
            async with semaphore:
                return await agent_instance.process_case(case)
        
        tasks = [bounded_process(case) for case in test_cases]
        responses = await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        
        # æŒ‡æ ‡è®¡ç®—
        success_count = sum(1 for r in responses if not r.get('error'))
        durations = [r.get('duration', 0) for r in responses if r.get('duration') is not None]
        
        if durations:
            results['latency']['min'] = min(durations)
            results['latency']['max'] = max(durations)
            results['latency']['avg'] = sum(durations) / len(durations)
            
            sorted_durations = sorted(durations)
            p95_index = int(len(sorted_durations) * 0.95)
            results['latency']['p95'] = sorted_durations[min(p95_index, len(sorted_durations) - 1)]
        
        results['throughput'] = len(test_cases) / total_time if total_time > 0 else 0
        results['success_rate'] = success_count / len(test_cases) if test_cases else 0
        
        return results

# ==========================================
# æ ¸å¿ƒç»„ä»¶ç±»å®šä¹‰
# ==========================================

class AsyncKnowledgeRetriever:
    """
    å¼‚æ­¥çŸ¥è¯†æ£€ç´¢å™¨ (é«˜æ€§èƒ½ç‰ˆ)
    ä¼˜åŒ–ç‚¹ï¼šç¼“å­˜é€»è¾‘å‰ç½®åˆ°å¼‚æ­¥å±‚ï¼Œä¸»çº¿ç¨‹å†…å­˜æŸ¥æ‰¾ (O(1))ï¼Œåªæœ‰ Cache Miss æ—¶æ‰è¿›å…¥çº¿ç¨‹æ± ã€‚
    """
    def __init__(self, vector_rag_tool, thread_pool_size: int = 4):
        self.vector_rag = vector_rag_tool
        self.logger = logging.getLogger("AsyncRetriever")
        self.thread_pool = ThreadPoolExecutor(max_workers=thread_pool_size)
        self.cache = {}  # ç®€å•çš„å†…å­˜ç¼“å­˜
        self.cache_ttl = 600  # 10åˆ†é’Ÿ
        self.cache_timestamps = {}
        
    async def retrieve(self, query: str, use_cache: bool = True) -> List[Dict]:
        """å¼‚æ­¥æ£€ç´¢çŸ¥è¯†"""
        if not self.vector_rag:
            return []

        cache_key = f"retrieve:{hash(query)}"
        
        # 1. ç¼“å­˜å±‚ (ä¸»çº¿ç¨‹éé˜»å¡å¿«é€Ÿè¿”å›)
        if use_cache:
            cached_result = self._get_from_cache(cache_key)
            if cached_result is not None:
                return cached_result
        
        # 2. è®¡ç®—å±‚ (çº¿ç¨‹æ± æ‰§è¡Œï¼Œé¿å…é˜»å¡ AsyncIO Loop)
        try:
            loop = asyncio.get_running_loop()
            
            def sync_retrieve():
                # è¿™æ˜¯ä¸€ä¸ªåŒæ­¥çš„ CPU/IO å¯†é›†å‹æ“ä½œ (å‘é‡è®¡ç®—/æ–‡ä»¶è¯»å–)
                return self.vector_rag.search(query)
            
            # Offload åˆ°çº¿ç¨‹æ± 
            results = await loop.run_in_executor(self.thread_pool, sync_retrieve)
            
            # 3. æ›´æ–°ç¼“å­˜
            if use_cache and results:
                self._set_to_cache(cache_key, results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"å¼‚æ­¥æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _get_from_cache(self, cache_key: str):
        """ä»ç¼“å­˜è·å– (åŒæ­¥æ“ä½œ)"""
        if cache_key in self.cache:
            timestamp = self.cache_timestamps.get(cache_key, 0)
            if time.time() - timestamp < self.cache_ttl:
                return self.cache[cache_key]
            else:
                # æƒ°æ€§åˆ é™¤è¿‡æœŸé¡¹
                del self.cache[cache_key]
                del self.cache_timestamps[cache_key]
        return None
    
    def _set_to_cache(self, cache_key: str, results: List[Dict]):
        """è®¾ç½®ç¼“å­˜"""
        self.cache[cache_key] = results
        self.cache_timestamps[cache_key] = time.time()
        
        # ç®€å•çš„å®¹é‡æ§åˆ¶é˜²æ­¢OOM
        if len(self.cache) > 2000:
            # éšæœºæ¸…ç† 10% æ—§æ•°æ® (ç®€åŒ–ç‰ˆLRU)
            keys = list(self.cache.keys())[:200]
            for k in keys:
                self.cache.pop(k, None)
                self.cache_timestamps.pop(k, None)

class EnhancedRouter:
    """
    å¢å¼ºå‹è·¯ç”±æ§åˆ¶å™¨
    ç­–ç•¥ï¼šæ­£åˆ™è§„åˆ™ (L1) -> è½»é‡æ¨¡å‹ (L2) -> ç½®ä¿¡åº¦æ£€æŸ¥ (L3)
    """
    def __init__(self, llm_client: Any, model_name: str = None):
        self.llm_client = llm_client
        self.model_name = model_name
        self.logger = logging.getLogger("EnhancedRouter")
        
        # é¢„ç¼–è¯‘æ­£åˆ™æ¨¡å¼ï¼Œæå‡æ€§èƒ½
        self.simple_patterns = [
            re.compile(r"^(ä½ å¥½|åœ¨å—|hi|hello|æ—©ä¸Šå¥½|æ™šä¸Šå¥½|åˆå®‰|æ™šå®‰)$", re.I),
            re.compile(r"^(è°¢è°¢|æ„Ÿè°¢|å†è§|æ‹œæ‹œ|ok|å¥½çš„|å¥½çš„å‘¢|å—¯å—¯)$", re.I),
            re.compile(r"^.{0,4}$"),  # è¶…çŸ­æ–‡æœ¬
            re.compile(r"^(è¯·é—®|ä½ å¥½|å“ˆå–½)[ï¼Œã€‚ï¼ï¼Ÿ]*$", re.I)  # ç¤¼è²Œæ€§å¼€å¤´
        ]
        
        self.complex_patterns = [
            re.compile(r"(æ€ä¹ˆ|å¦‚ä½•|ä¸ºä»€ä¹ˆ|ä»€ä¹ˆåŸå› |æ€ä¹ˆåŠ|æ€ä¹ˆè§£å†³|æ€ä¹ˆå¤„ç†)", re.I),
            re.compile(r"(é”™è¯¯|æ•…éšœ|å¼‚å¸¸|æŠ¥é”™|bug|é—®é¢˜|issue)", re.I),
            re.compile(r"(é…ç½®|è®¾ç½®|å®‰è£…|éƒ¨ç½²|æ­å»º|å¯åŠ¨|è¿è¡Œ)", re.I),
            re.compile(r"(api|æ¥å£|è°ƒç”¨|è¯·æ±‚|å“åº”|è¿”å›)", re.I)
        ]
        
        self.stats = defaultdict(int)
    
    async def classify(self, query: str) -> str:
        """æ‰§è¡Œè·¯ç”±åˆ†ç±»"""
        query = query.strip()
        if not query:
            return 'SIMPLE' 
        
        # L1: è§„åˆ™è·¯ç”± (0å»¶è¿Ÿ)
        for pattern in self.simple_patterns:
            if pattern.search(query):
                self.stats['rule_hit_simple'] += 1
                return 'SIMPLE'
        
        for pattern in self.complex_patterns:
            if pattern.search(query):
                self.stats['rule_hit_complex'] += 1
                return 'COMPLEX'
        
        # L2: æ¨¡å‹è·¯ç”±
        if self.model_name and self.llm_client:
            try:
                response = await self.llm_client.call_model(
                    model=self.model_name,
                    messages=[{"role": "user", "content": INTENT_ROUTER_PROMPT.format(query=query)}],
                    temperature=0.0,
                    max_tokens=10
                )
                
                intent = 'SIMPLE' if '[SIMPLE]' in response else 'COMPLEX'
                self.stats[f'model_{intent.lower()}'] += 1
                return intent
                
            except Exception as e:
                self.logger.warning(f"è·¯ç”±æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}ï¼Œé™çº§ä¸ºè§„åˆ™åˆ¤æ–­")
        
        # L3: é™çº§å…œåº•
        if len(query) > 50 or '?' in query or 'ï¼Ÿ' in query:
            self.stats['fallback_complex'] += 1
            return 'COMPLEX'
        
        self.stats['fallback_simple'] += 1
        return 'SIMPLE'
    
    async def check_health(self) -> bool:
        """ç»„ä»¶çº§å¥åº·æ£€æŸ¥"""
        return True

class SmartConcurrencyManager:
    """æ™ºèƒ½å¹¶å‘ç®¡ç†å™¨"""
    def __init__(self, simple_limit=20, complex_limit=5):
        self.semaphores = {
            'SIMPLE': asyncio.Semaphore(simple_limit),
            'COMPLEX': asyncio.Semaphore(complex_limit),
            'UNKNOWN': asyncio.Semaphore(5)
        }
        self.limits = {'SIMPLE': simple_limit, 'COMPLEX': complex_limit}
        self.usage_stats = defaultdict(int)
    
    def get_semaphore(self, mode: str) -> asyncio.Semaphore:
        """è·å–å¯¹åº”æ¨¡å¼çš„ä¿¡å·é‡"""
        semaphore = self.semaphores.get(mode, self.semaphores['UNKNOWN'])
        self.usage_stats[mode] += 1
        return semaphore
    
    def get_stats(self) -> Dict:
        """è·å–å¹¶å‘ç»Ÿè®¡"""
        stats = {}
        for mode, sem in self.semaphores.items():
            available = sem._value
            limit = self.limits.get(mode, 5)
            stats[mode] = {
                'available': available,
                'limit': limit,
                'in_use': limit - available,
                'usage_count': self.usage_stats.get(mode, 0)
            }
        return stats

# ==========================================
# ä¸» Agent ç±»å®šä¹‰
# ==========================================

class EnhancedCustomerServiceAgent:
    """
    å…¨åŠŸèƒ½å¢å¼ºç‰ˆæ™ºèƒ½å®¢æœ Agent (V5.4)
    """
    
    def __init__(self, config: Dict = None, llm_client: Any = None):
        """
        åˆå§‹åŒ– Agent
        :param config: é…ç½®å­—å…¸
        :param llm_client: å¯é€‰ï¼Œä¾èµ–æ³¨å…¥ LLM å®¢æˆ·ç«¯ (ç”¨äºæµ‹è¯• Mock)
        """
        self.config = config or {}
        self._init_logging()
        
        self.logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– V5.4 æ——èˆ°æ²»ç†ç‰ˆ Agent...")
        
        # 0. é…ç½®æ ¡éªŒ
        try:
            ConfigValidator.validate(self.config)
            self.logger.info("âœ… é…ç½®æ ¡éªŒé€šè¿‡")
        except ValueError as e:
            self.logger.error(f"âŒ é…ç½®é”™è¯¯: {e}")
            raise
        
        # 1. æ ¸å¿ƒæ¨¡å‹å®¢æˆ·ç«¯ (æ”¯æŒä¾èµ–æ³¨å…¥)
        if llm_client:
            self.llm_client = llm_client
            self.logger.info("âœ… ä½¿ç”¨æ³¨å…¥çš„ LLM å®¢æˆ·ç«¯")
        else:
            try:
                self.llm_client = DeepSeekClient()
            except Exception as e:
                self.logger.error(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                raise
        
        # 2. æå–æ¨¡å‹é…ç½®
        default_models = {
            'router': 'deepseek/deepseek-v3.2',
            'simple': 'deepseek/deepseek-v3.2',
            'complex': 'deepseek/deepseek-v3.2-think',
            'verifier': 'deepseek/deepseek-v3.2'
        }
        self.model_config = {**default_models, **self.config.get('models', {})}
        
        # 3. æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.router = EnhancedRouter(self.llm_client, model_name=self.model_config['router'])
        self.concurrency_mgr = SmartConcurrencyManager(
            simple_limit=self.config.get('concurrency_simple', 20),
            complex_limit=self.config.get('concurrency_complex', 5)
        )
        
        # 4. å·¥å…·é›†åˆå§‹åŒ–
        self._init_tools()
        
        # 5. è‡ªåŠ¨é¢„çƒ­
        if self.config.get('auto_warmup', True):
            asyncio.create_task(self._comprehensive_warmup())
        
        self.start_time = datetime.now()
        self.request_counter = 0
        self.logger.info("âœ… Agent åˆå§‹åŒ–å®Œæˆ")
    
    def _init_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger("EnhancedAgent-V5.4")
    
    def _init_tools(self):
        """åˆå§‹åŒ–å·¥å…·é›†"""
        # ç¼“å­˜
        try:
            self.cache_manager = AdvancedCacheManager(self.config.get('cache', {}))
        except Exception:
            self.logger.warning("ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨å†…å­˜ç¼“å­˜")
            self.cache_manager = None
        
        # RAG
        try:
            rag_cfg = self.config.get('rag_config', {'top_k': 3, 'rerank_threshold': 0.1})
            kb_path = self.config.get('knowledge_base')
            self.vector_rag = OptimizedVectorRAGTool(knowledge_base_path=kb_path, config=rag_cfg)
        except Exception as e:
            self.logger.error(f"âŒ RAGå·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")
            self.vector_rag = None
        
        # å¼‚æ­¥æ£€ç´¢å™¨
        self.async_retriever = AsyncKnowledgeRetriever(self.vector_rag)
        
        # ç›‘æ§ä¸å¤–éƒ¨å·¥å…·
        self.metrics = MetricsTool()
        self.feishu_tool = FeishuTool()
        self.apifox_tool = ApifoxTool()
        self.monitor_tool = MonitorTool()
    
    async def _comprehensive_warmup(self):
        """å…¨é“¾è·¯é¢„çƒ­"""
        self.logger.info("ğŸ”¥ å¼€å§‹å…¨é“¾è·¯é¢„çƒ­...")
        tasks = []
        
        # é¢„çƒ­ RAG
        if self.vector_rag:
            tasks.append(self.async_retriever.retrieve("ç³»ç»Ÿé¢„çƒ­æŸ¥è¯¢", use_cache=False))
        
        # é¢„çƒ­ LLM
        tasks.append(self.llm_client.call_model(
            model=self.model_config['simple'],
            messages=[{"role": "user", "content": "ping"}],
            temperature=0.0,
            max_tokens=5
        ))
        
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            success = sum(1 for r in results if not isinstance(r, Exception))
            self.logger.info(f"âœ… é¢„çƒ­å®Œæˆï¼ŒæˆåŠŸ: {success}/{len(tasks)}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ é¢„çƒ­é”™è¯¯: {e}")
    
    # ==========================================
    # è¿ç»´æ¥å£ (Health & Benchmark)
    # ==========================================
    
    async def _check_llm_connectivity(self) -> Dict:
        """LLM è¿æ¥æµ‹è¯•"""
        try:
            start = time.time()
            await self.llm_client.call_model(
                model=self.model_config['simple'],
                messages=[{"role": "user", "content": "ping"}],
                max_tokens=5
            )
            return {'connected': True, 'latency_ms': (time.time() - start) * 1000}
        except Exception as e:
            return {'connected': False, 'error': str(e)}
    
    async def _check_rag_health(self) -> Dict:
        """RAGå¥åº·æ£€æŸ¥ (æ”¯æŒæ‡’åŠ è½½æ¢æµ‹ä¸æ·±åº¦æ£€æŸ¥)"""
        if not self.vector_rag:
            return {'ready': False, 'reason': 'Not Initialized'}
        
        try:
            # 1. ä¼˜å…ˆå°è¯•è°ƒç”¨å·¥å…·è‡ªå¸¦çš„æ£€æŸ¥æ–¹æ³•
            if hasattr(self.vector_rag, 'check_health') and callable(self.vector_rag.check_health):
                if asyncio.iscoroutinefunction(self.vector_rag.check_health):
                    return await self.vector_rag.check_health()
                else:
                    return self.vector_rag.check_health()

            # 2. é™çº§ï¼šæ‰‹åŠ¨æ£€æŸ¥
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            kb_path = getattr(self.vector_rag, 'knowledge_base_path', None)
            file_exists = False
            if kb_path:
                file_exists = Path(kb_path).exists()
            
            # æ£€æŸ¥ç´¢å¼•çŠ¶æ€ (é˜²å¾¡å¼ç¼–ç¨‹)
            index_ready = False
            chunks = 0
            if hasattr(self.vector_rag, 'knowledge_chunks'):
                chunks = len(self.vector_rag.knowledge_chunks)
                index_ready = chunks > 0
            elif hasattr(self.vector_rag, 'lazy_load') and self.vector_rag.lazy_load:
                # å¦‚æœæ˜¯æ‡’åŠ è½½ä¸”æœªåˆå§‹åŒ–ï¼Œä½†æ–‡ä»¶å­˜åœ¨ï¼Œè§†ä¸ºå°±ç»ª
                if file_exists:
                    index_ready = True
            
            return {
                'ready': file_exists and index_ready,
                'file_exists': file_exists,
                'index_loaded': chunks > 0,
                'chunks_count': chunks
            }
        except Exception as e:
            return {'ready': False, 'error': str(e)}
    
    async def check_health(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥æ¥å£"""
        checks = await asyncio.gather(
            self._check_llm_connectivity(),
            self._check_rag_health(),
            self.router.check_health()
        )
        llm_h, rag_h, router_h = checks
        
        is_healthy = llm_h['connected'] and rag_h['ready'] and router_h
        
        return {
            'status': 'healthy' if is_healthy else 'degraded',
            'uptime': str(datetime.now() - self.start_time),
            'components': {
                'llm': llm_h,
                'rag': rag_h,
                'router': {'ready': router_h}
            }
        }
    
    async def run_benchmark(self, test_cases: List[Dict], concurrency: int = 10) -> Dict:
        """æ‰§è¡ŒåŸºå‡†æµ‹è¯•"""
        self.logger.info(f"ğŸ“‰ æ‰§è¡ŒåŸºå‡†æµ‹è¯• (N={len(test_cases)}, C={concurrency})...")
        return await PerformanceBenchmark.run_benchmark(self, test_cases, concurrency)
    
    # ==========================================
    # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
    # ==========================================
    
    async def _handle_simple_chat(self, query: str) -> str:
        try:
            return await self.llm_client.call_model(
                model=self.model_config['simple'],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçƒ­æƒ…ä¸“ä¸šçš„å®¢æœã€‚è¯·ç®€çŸ­ç¤¼è²Œåœ°å›å¤ã€‚"},
                    {"role": "user", "content": query}
                ],
                temperature=0.7,
                max_tokens=200
            )
        except Exception:
            return "æ‚¨å¥½ï¼Œç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    async def _handle_complex_query(self, user_query: str, context: Dict) -> str:
        # 1. å¼‚æ­¥æ£€ç´¢ (Cache -> ThreadPool)
        knowledge = await self.async_retriever.retrieve(user_query)
        
        if not knowledge:
            return "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æš‚æ—¶æ²¡æœ‰ç›¸å…³è®°å½•ã€‚"
        
        # 2. æ„å»º Prompt
        ctx_str = "\n".join([f"[å‚è€ƒ{i+1}] {r['text']}" for i, r in enumerate(knowledge[:3])])
        prompt = ENHANCED_RAG_PROMPT.format(
            context_str=ctx_str,
            query=user_query,
            system_status=f"API: {context.get('api_status', 'OK')}"
        )
        
        # 3. ç”Ÿæˆä¸éªŒè¯
        for _ in range(3):
            try:
                reply = await self.llm_client.call_model(
                    model=self.model_config['complex'],
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3
                )
                
                # éªŒè¯é€»è¾‘ (å¯é€‰)
                # is_valid = await self._verify_response(user_query, reply, knowledge)
                
                return reply
            except Exception:
                await asyncio.sleep(0.5)
        
        return "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"
    
    async def process_case(self, case_data: Dict) -> Dict[str, Any]:
        """å¤„ç†å…¥å£"""
        case_id = case_data.get('case_id', f"req_{self.request_counter}")
        query = case_data.get('user_query', '')
        self.request_counter += 1
        
        start_t = time.time()
        result = {'case_id': case_id, 'reply': '', 'mode': 'PENDING', 'error': None}
        
        try:
            # 1. å‘Šè­¦æ£€æŸ¥
            if self._should_trigger_alert(case_data):
                result['alerts'] = await self._trigger_alerts(case_id, case_data)
            
            # 2. è·¯ç”±
            mode = await self.router.classify(query)
            result['mode'] = mode
            
            # 3. æ‰§è¡Œ
            sem = self.concurrency_mgr.get_semaphore(mode)
            async with sem:
                if mode == 'SIMPLE':
                    result['reply'] = await self._handle_simple_chat(query)
                else:
                    result['reply'] = await self._handle_complex_query(query, case_data)
            
            result['duration'] = time.time() - start_t
            self.metrics.record_latency(f"process_{mode}", result['duration'])
            
        except Exception as e:
            self.logger.error(f"å¤„ç†å¼‚å¸¸: {e}")
            result['error'] = str(e)
            result['reply'] = "ç³»ç»Ÿé”™è¯¯"
        
        return result

    def _should_trigger_alert(self, data: Dict) -> bool:
        return 'error' in str(data.get('api_status', '')).lower()

    async def _trigger_alerts(self, cid: str, data: Dict) -> List[str]:
        # ç®€åŒ–ç‰ˆï¼šä»…ä½œä¸ºæ¼”ç¤º
        return []

    async def process_batch(self, input_file: str, output_file: str) -> Dict:
        """æ‰¹é‡å¤„ç†å…¥å£"""
        with open(input_file, 'r', encoding='utf-8') as f:
            cases = json.load(f)
        
        tasks = [self.process_case(c) for c in cases]
        results = await asyncio.gather(*tasks)
        
        out_path = Path(output_file)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        with open(out_path, 'w') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
            
        return {'total': len(cases), 'success': sum(1 for r in results if not r['error'])}
    
    def get_system_status(self) -> Dict:
        return {
            'version': '5.4.0',
            'uptime': str(datetime.now() - self.start_time),
            'requests': self.request_counter
        }
    
    def generate_performance_report(self) -> str:
        return "Performance Report V5.4 Generated."

# ==========================================
# å•å…ƒæµ‹è¯• (Mock Support)
# ==========================================

class MockLLMClient:
    """ç”¨äºæµ‹è¯•çš„ Mock LLM å®¢æˆ·ç«¯"""
    async def call_model(self, model, messages, **kwargs):
        content = messages[-1]['content']
        if "INTENT" in content or "åˆ¤æ–­æ„å›¾" in content:
            if "ä½ å¥½" in content: return "[SIMPLE]"
            return "[COMPLEX]"
        if "ping" in content:
            return "pong"
        return "Mock Response"

class AgentUnitTest:
    """æ™ºèƒ½å®¢æœAgentå•å…ƒæµ‹è¯•å¥—ä»¶"""
    @staticmethod
    async def run_smoke_tests():
        print("ğŸ§ª å¼€å§‹å†’çƒŸæµ‹è¯• (Mockç¯å¢ƒ)...")
        results = []
        
        try:
            # 1. æ³¨å…¥ Mock å®¢æˆ·ç«¯
            mock_client = MockLLMClient()
            config = {
                'concurrency_simple': 5,
                'auto_warmup': False,
                'rag_config': {'top_k': 1},
                'models': {
                    'router': 'mock_router',
                    'simple': 'mock_simple',
                    'complex': 'mock_complex',
                    'verifier': 'mock_verifier'
                }
            }
            agent = EnhancedCustomerServiceAgent(config, llm_client=mock_client)
            
            # æµ‹è¯• 1: å¥åº·æ£€æŸ¥
            print("  Test 1: Health Check...", end="")
            health = await agent.check_health()
            assert health['components']['llm']['connected'] is True
            print("âœ… PASS")
            
            # æµ‹è¯• 2: ç®€å•è·¯ç”±
            print("  Test 2: Simple Routing...", end="")
            res = await agent.process_case({'user_query': 'ä½ å¥½'})
            assert res['mode'] == 'SIMPLE'
            print("âœ… PASS")
            
            results.append("ALL PASS")
            
        except Exception as e:
            print(f"âŒ FAIL: {e}")
            traceback.print_exc()
            results.append("FAIL")
            
        return results

if __name__ == "__main__":
    async def main():
        # è¿è¡Œæµ‹è¯•
        await AgentUnitTest.run_smoke_tests()
        
        # å¯åŠ¨æ¼”ç¤º
        print("\nğŸš€ å¯åŠ¨æ¼”ç¤ºæœåŠ¡...")
        try:
            agent = EnhancedCustomerServiceAgent()
            print(f"System Status: {agent.get_system_status()}")
        except Exception as e:
            print(f"Startup skipped (Missing config/key): {e}")

    asyncio.run(main())
